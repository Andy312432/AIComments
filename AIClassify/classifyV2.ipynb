{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# ç¢ºèªæ˜¯å¦æœ‰å¯ç”¨çš„GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®€å–CSVè³‡æ–™ï¼Œå‡è¨­åŒ…å« \"text\" èˆ‡ \"label\" æ¬„ä½\n",
    "df = pd.read_csv(\"referance.csv\")\n",
    "\n",
    "# å‡è¨­9å€‹åˆ†é¡æ¨™ç±¤æ˜¯ä»¥ä¸‹ï¼ˆè«‹æ ¹æ“šå¯¦éš›æƒ…æ³èª¿æ•´ï¼‰\n",
    "categories = [\n",
    "    \"æ‰¹è©•é¸æ°‘èˆ‡å‹•å“¡\",\n",
    "    \"åœ°æ–¹æˆ–æ”¿é»¨æ”¿æ²»æ‰¹è©•\",\n",
    "    \"äººèº«æ”»æ“Šè¨€è«–\",\n",
    "    \"ç½·å…ç«‹å ´è¡¨æ…‹\",\n",
    "    \"æ”¿ç­–èˆ‡ç¨‹åºè¨è«–\",\n",
    "    \"æŠµåˆ¶èˆ‡è¡Œå‹•å‘¼ç±²\",\n",
    "    \"å‰µæ„è¡¨é”èˆ‡è«·åˆº\",\n",
    "    \"åœ‹å®‰åŠè³£å°æŒ‡æ§\",\n",
    "    \"äº‹ä»¶å¼•ç”¨è«–è¿°æˆ–åŠ æ²¹çš„è¨€è«–\",\n",
    "    \"å…¶ä»–\"\n",
    "]\n",
    "\n",
    "# å°‡æ–‡å­—æ¨™ç±¤è½‰æˆæ•¸å­—ID\n",
    "label2id = {cat: idx for idx, cat in enumerate(categories)}\n",
    "id2label = {idx: cat for cat, idx in label2id.items()}\n",
    "\n",
    "df['label_id'] = df['label'].apply(lambda x: label2id[x])  # å°‡æ–‡å­—æ¨™ç±¤è½‰æ•¸å­—\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨“ç·´ç­†æ•¸ï¼š 283\n",
      "æ¸¬è©¦ç­†æ•¸ï¼š 71\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label_id'], random_state=42)\n",
    "\n",
    "print(\"è¨“ç·´ç­†æ•¸ï¼š\", len(train_df))\n",
    "print(\"æ¸¬è©¦ç­†æ•¸ï¼š\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"hfl/chinese-macbert-large\"  # ä½¿ç”¨æ›´å¤§çš„é è¨“ç·´æ¨¡å‹\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# å°‡æ–‡å­—è³‡æ–™è½‰æˆ BERT ç›¸å®¹çš„è¼¸å…¥æ ¼å¼\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"longest\",     # ä¹Ÿå¯ä»¥æ”¹æˆ longestï¼Œè‹¥æƒ³å‹•æ…‹paddingå¯ä½¿ç”¨ batched æ–¹å¼\n",
    "        truncation=True,\n",
    "        pad_to_max_length=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8b360580404ed8b8ba5068a1447179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2e453fb2d042168308304d8d4a25d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/71 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'label_id']])\n",
    "test_dataset  = Dataset.from_pandas(test_df[['text', 'label_id']])\n",
    "\n",
    "# é€²è¡Œ Tokenize\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# ç§»é™¤å¤šé¤˜æ¬„ä½ï¼Œä¸¦æŒ‡å®šç‰¹å¾µåç¨±\n",
    "train_dataset = train_dataset.remove_columns([\"text\"])\n",
    "train_dataset = train_dataset.rename_column(\"label_id\", \"labels\")\n",
    "train_dataset.set_format(\"torch\")\n",
    "\n",
    "test_dataset = test_dataset.remove_columns([\"text\"])\n",
    "test_dataset = test_dataset.rename_column(\"label_id\", \"labels\")\n",
    "test_dataset.set_format(\"torch\")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-macbert-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = 10  # åˆ†é¡æ•¸é‡\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "# å°‡æ¨¡å‹ç§»å‹•åˆ°GPU\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andyw\\Desktop\\æ¡‘é‘¼çš„åˆ†é¡\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",       # æ¯å€‹ epoch çµæŸå¾Œåšè©•ä¼°\n",
    "    save_strategy=\"epoch\",            # æ¯å€‹ epoch å„²å­˜ä¸€æ¬¡æ¨¡å‹ (å¯æ ¹æ“šéœ€æ±‚èª¿æ•´)\n",
    "    num_train_epochs=5,               # ä»¥è³‡æ–™é‡å°‘ç‚ºå‰æï¼Œå…ˆè©¦è©¦ 5 epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,    # è¦–ç¡¬é«”èª¿æ•´\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_dir=\"./logs\",             # è¨“ç·´éç¨‹ç´€éŒ„\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,       # åœ¨æœ€å¾Œè¼‰å…¥æœ€å¥½çš„æ¨¡å‹æ¬Šé‡\n",
    "    report_to=\"none\"                  # ç¦ç”¨å ±å‘Šåˆ°ä»»ä½•å¹³å°\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\"accuracy\": acc[\"accuracy\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_6340\\1215538910.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72' max='355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 72/355 09:21 < 37:50, 0.12 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.129800</td>\n",
       "      <td>1.867146</td>\n",
       "      <td>0.323944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m trainer = Trainer(\n\u001b[32m      2\u001b[39m     model=model,\n\u001b[32m      3\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# å–å¾—æœ€ä½³æ¨¡å‹å­˜æ”¾çš„ checkpoint è·¯å¾‘\u001b[39;00m\n\u001b[32m     13\u001b[39m best_checkpoint_dir = trainer.state.best_model_checkpoint\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\æ¡‘é‘¼çš„åˆ†é¡\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2241\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2239\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\æ¡‘é‘¼çš„åˆ†é¡\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2639\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2636\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2638\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2639\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2642\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2643\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\æ¡‘é‘¼çš„åˆ†é¡\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3092\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[39m\n\u001b[32m   3089\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3091\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3092\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3093\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\æ¡‘é‘¼çš„åˆ†é¡\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3194\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3190\u001b[39m \u001b[38;5;28mself\u001b[39m.save_model(output_dir, _internal_call=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_only_model:\n\u001b[32m   3193\u001b[39m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3194\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3195\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_scaler(output_dir)\n\u001b[32m   3196\u001b[39m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\æ¡‘é‘¼çš„åˆ†é¡\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3316\u001b[39m, in \u001b[36mTrainer._save_optimizer_and_scheduler\u001b[39m\u001b[34m(self, output_dir)\u001b[39m\n\u001b[32m   3311\u001b[39m     save_fsdp_optimizer(\n\u001b[32m   3312\u001b[39m         \u001b[38;5;28mself\u001b[39m.accelerator.state.fsdp_plugin, \u001b[38;5;28mself\u001b[39m.accelerator, \u001b[38;5;28mself\u001b[39m.optimizer, \u001b[38;5;28mself\u001b[39m.model, output_dir\n\u001b[32m   3313\u001b[39m     )\n\u001b[32m   3314\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m   3315\u001b[39m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3316\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3318\u001b[39m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[32m   3319\u001b[39m is_deepspeed_custom_scheduler = \u001b[38;5;28mself\u001b[39m.is_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m   3320\u001b[39m     \u001b[38;5;28mself\u001b[39m.lr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[32m   3321\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\æ¡‘é‘¼çš„åˆ†é¡\\.venv\\Lib\\site-packages\\torch\\serialization.py:944\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m    943\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\æ¡‘é‘¼çš„åˆ†é¡\\.venv\\Lib\\site-packages\\torch\\serialization.py:1216\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[39m\n\u001b[32m   1214\u001b[39m     storage = storage.cpu()\n\u001b[32m   1215\u001b[39m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1216\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# å–å¾—æœ€ä½³æ¨¡å‹å­˜æ”¾çš„ checkpoint è·¯å¾‘\n",
    "best_checkpoint_dir = trainer.state.best_model_checkpoint\n",
    "print(\"Best model checkpoint folder:\", best_checkpoint_dir)\n",
    "\n",
    "# å¯ä»¥æŠŠæœ€ä½³æ¨¡å‹å¦å­˜åˆ°æŸå€‹è³‡æ–™å¤¾ï¼Œä¾‹å¦‚ \"./best_model\"\n",
    "best_model_dir = \"./best_model\"\n",
    "trainer.save_model(best_model_dir)\n",
    "print(f\"Best model is now saved to: {best_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dir = \"./best_model_\"\n",
    "model_name = \"hfl/chinese-roberta-wwm-ext\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(best_model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(best_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "all_list = []\n",
    "#for i in range(48):\n",
    "for i in range(2):\n",
    "    #input_json_path = f\"C://Users//andyw//Desktop//æ¡‘é‘¼çš„åˆ†é¡//fb_comments//comments//result_{i}.json\"\n",
    "    input_json_path = f\"C://Users//andyw//Desktop//æ¡‘é‘¼çš„åˆ†é¡//fb_comments//result_video{i}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            file_data = json.load(f) \n",
    "\n",
    "        row_list = []\n",
    "        for element in file_data:\n",
    "            row_list.append({\n",
    "                \"å…§å®¹\": element[0],\n",
    "                \"æ™‚é–“\": element[1]\n",
    "            })\n",
    "        all_list.append(row_list)\n",
    "    except:#not found\n",
    "        all_list.append([])\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡æœ¬: é€£ç½²æ›¸å¯ä»¥è¦ªé€å—ï¼Ÿ\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: é«˜å±±é¢¨ æ‚¨å¥½ï¼Œç›®å‰ç¶­æŒå»ºè­°å¯„ä»¶åˆ°éƒµæ”¿ä¿¡ç®±ï¼Œä½†æé†’å¯ä»¥ä½¿ç”¨æ›è™Ÿçš„æ–¹å¼å¯„å‡ºï¼ç›¸å°ä¿éšœï¼\n",
      " -> é æ¸¬åˆ†é¡: äº‹ä»¶å¼•ç”¨è«–è¿°æˆ–åŠ æ²¹çš„è¨€è«–\n",
      "\n",
      "æ–‡æœ¬: ç½·å®Œå‚…ã€‚å»èŠ±è“®ç©\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: é»ƒåœ‹æ¬½ æ­¡è¿å”·\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: å¾®å…‰ shimmer.tw åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: ä¸Šè¡—é ­ç™¼é€£ç½²æ›¸æœƒä¸æœƒæ›´æœ‰æ•ˆç›Šï¼Ÿè®“æ›´å¤šäººçŸ¥é“æ›´å¥½å–å¾—ï¼Ÿç•¢ç«Ÿæœ‰å¹´ç´€çš„äººä¸ä¸€å®šæœƒåˆ—å°\n",
      " -> é æ¸¬åˆ†é¡: åœ‹å®‰åŠè³£å°æŒ‡æ§\n",
      "\n",
      "æ–‡æœ¬: æ´ªéœè•™ æœƒæœ‰çš„\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: æ´ªéœè•™ è¦åŠƒä¸­\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: é€™é‚Šæœ‰4ä»½ å¾ˆå¿«å¯„å‡º\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: Chen Hong Hong è¬è¬\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: Chen Hong Hong \n",
      " -> é æ¸¬åˆ†é¡: æŠµåˆ¶èˆ‡è¡Œå‹•å‘¼ç±²\n",
      "\n",
      "æ–‡æœ¬: é€£ç½²æ›¸æ€éº¼é ˜å–???\n",
      " -> é æ¸¬åˆ†é¡: äº‹ä»¶å¼•ç”¨è«–è¿°æˆ–åŠ æ²¹çš„è¨€è«–\n",
      "\n",
      "æ–‡æœ¬: è¶™ç‰èŠ³ éœ€è¦è‡ªè¡Œä¸‹è¼‰åˆ—å°https://drive.google.com/....../1IevLDmwLJx....../view......\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: åŠ æ²¹åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: èŠ±è“®åŠ æ²¹ï¼ï¼ï¼\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: æ¯å¤©ä¾†é»å¸ƒè¾²èªå•Šï¼mapasnava Bunun saikin ä¸€èµ·åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: äº‹ä»¶å¼•ç”¨è«–è¿°æˆ–åŠ æ²¹çš„è¨€è«–\n",
      "\n",
      "æ–‡æœ¬: èŠ±è“®äºº åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: Happy Hsieh èŠ±è“®äººåŠ æ²¹å°ç£åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: è«‹å•ç¾åœ¨é·æˆ¶ç±åˆ°èŠ±è“®ä¾†å¾—åŠå—ï¼Ÿï¿¼\n",
      " -> é æ¸¬åˆ†é¡: äº‹ä»¶å¼•ç”¨è«–è¿°æˆ–åŠ æ²¹çš„è¨€è«–\n",
      "\n",
      "æ–‡æœ¬: åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: åŠ æ²¹åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: æ±Ÿæ¬£ç‡• åŠ æ²¹åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: åŠ æ²¹ï¼Œå·²åˆ†äº«ï¼Œæˆ‘å¾ˆå‰ä¸€æ‰¹6æœˆçš„ä¸çŸ¥é“æœ‰æ²’æœ‰é †åˆ©æ”¶åˆ° XD\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: Hsiao Tung Chiu ä¸€å®šæ˜¯æœ‰çš„\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: è«‹å•æ—©å…ˆå…«ç‚¯é‚£æ¬¡æœ‰é€£ç½²äº†ï¼Œé‚„è¦å†ç°½ä¸€æ¬¡å—?\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: èŠ±è“®äºº åŠ æ²¹!\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: ç”¨å¯„é€çš„æ–¹å¼æœƒä¸æœƒå› ç‚ºæŸç¨®åŸå› æ²’é€é”å•Š??? ç•¢ç«Ÿå¹³ä¿¡æœ¬ä¾†å°±æ²’ä¿è­‰ä¸€å®šé€é”\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: è¨±å°‘éŠ“ ä¸ç”¨æ“”å¿ƒï¼çœŸçš„æ“”å¿ƒå¯ä»¥ç”¨ã€Œæ›è™Ÿã€æˆ–ã€Œé™æ›ã€æ–¹å¼å¯„å‡ºï¼\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: æˆ‘ä¸æ˜¯èŠ±è“®äººï¼Œå¸Œæœ›ä½ å€‘ç‚ºäº†å…¨æ°‘ï¼Œå¤§å®¶çœ‹åœ¨çœ¼è£¡ï¼Œé ç¥æˆåŠŸã€‚é€™å€‹æ»¯å°çš„å…±ç”¢é»¨ä¸é™¤ï¼Œå°ç£é›£å¹³éœã€‚è‡ªå·±åšéŒ¯äº‹è¢«é—œï¼Œç¾åœ¨æŠŠä»‡æ¨é™„åŠ åœ¨å…¨æ°‘\n",
      " -> é æ¸¬åˆ†é¡: åœ‹å®‰åŠè³£å°æŒ‡æ§\n",
      "\n",
      "æ–‡æœ¬: 60æ—¥æ˜¯åˆ°ä»€éº¼æ™‚å€™ï¼Ÿéå¹´æ‹¿å›å»çµ¦æˆ‘çˆ¸åª½ç°½\n",
      " -> é æ¸¬åˆ†é¡: äº‹ä»¶å¼•ç”¨è«–è¿°æˆ–åŠ æ²¹çš„è¨€è«–\n",
      "\n",
      "æ–‡æœ¬: è½èªªç¬¬äºŒéšæ®µå¾ˆé›£\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: å¹«æ“´æ•£åˆ°æ‰€æœ‰ç½·å…åœ˜éšŠä»¥ä¸‹è½‰åˆ†äº«è‡ªã€ŒKazuma Kawazoe ã€è„†è²¼æ–‡ï¼šå‰›å‰›åšäº† A3/A4 æ¯”ä¾‹çš„å‘Šç¤ºç‰Œå¦‚æœæœ‰äººä¾†ç½·å…æ´»å‹•é¨·æ“¾ï¼Œæˆ–è¨±å¯æ´¾ä¸Šç”¨å ´/æ­¡è¿è‡ªç”±å–ç”¨ï¼Œæˆ‘é›²ç«¯è³‡æ–™å¤¾è¨­å®šç‚ºå…¬é–‹ï¼šhttps://drive.google.com/....../1JXwjMnp6mFsqnIBrT1stG....../æˆ‘æ²’æŸ¥åˆ°æ‰€æœ‰çš„å…¬æ°‘ç½·å…å¸³è™Ÿï¼Œå…ˆ tag å„ä½ï½å¦‚æœå¯ä»¥è«‹å¹«æˆ‘æ“´æ•£ï¼Œè¬è¬ï¼@byebyecow777@shimmer.taiwan@songxin.recall2025@wanghongwei2025gg@daanreboothttps://www.threads.net/@youngandayu/post/DEbUR4Wzc9K......#è¶Šé™ç¸®äººæ°‘ç½·å…æ¬Šä»£è¡¨è—ç™½è¶Šå®³æ€•#å„è·¯ç½·å…åœ˜éšŠä¸€èµ·åŠ æ²¹#è—ç™½ç´…éƒ½å€’å°ç£æ‰æœƒå¥½\n",
      " -> é æ¸¬åˆ†é¡: äº‹ä»¶å¼•ç”¨è«–è¿°æˆ–åŠ æ²¹çš„è¨€è«–\n",
      "\n",
      "æ–‡æœ¬: å¤§å®¶ä¸€èµ·åŠªåŠ›åŠ æ²¹ï¼\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: èŠ±è“®äººç½·å…è¡èµ·ä¾†ï¼Œçµ¦ä¸‹ä¸€ä»£ä¸€å€‹ä¸è¢«å‚…å´‘èçµ±æ²»çš„æ©Ÿæœƒ\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: åœ¨é‚£é‚Šå¯ä»¥æ‹¿é€£ç½²æ›¸\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: ä½™æ–‡æ…¶  å¯è‡ªè¡Œä¸‹è¼‰åˆ—å°https://drive.google.com/....../1IevLDmwLJx....../view......\n",
      " -> é æ¸¬åˆ†é¡: äº‹ä»¶å¼•ç”¨è«–è¿°æˆ–åŠ æ²¹çš„è¨€è«–\n",
      "\n",
      "æ–‡æœ¬: ä¸èƒ½è·Ÿå…¶ä»–ç¸£å¸‚ä¸€æ¨£å°å¥½ç›´æ¥é ˜å–ç¾åœ¨ç°½åå°±å®Œæˆäº†ï¼Œå¿«é€Ÿåˆæ–¹ä¾¿\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: Hugo Chen å¯è‡ªè¡Œä¸‹è¼‰åˆ—å°æœƒæ›´å¿«è·Ÿæ–¹ä¾¿å–”ï¼è‡³æ–¼å¤§é‡å°è£½æŸå€‹ç¨‹åº¦ä¸Šå°æ²’æœ‰ä»»ä½•é‡‘æ´çš„åœ˜éšŠä¾†èªªä¹Ÿæœƒæ˜¯ä¸€é …è² æ“”ï¼â€¦â€¦ æŸ¥çœ‹æ›´å¤š\n",
      " -> é æ¸¬åˆ†é¡: åœ‹å®‰åŠè³£å°æŒ‡æ§\n",
      "\n",
      "å·²å°‡çµæœå„²å­˜è‡³ï¼šC://Users//andyw//Desktop//æ¡‘é‘¼çš„åˆ†é¡//result0.xlsx\n",
      "æ–‡æœ¬: æ­¡è¿å¤§å®¶ä¸€èµ·åˆ†äº«å‡ºå»\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: å¤§å®¶ä¸€èµ·åˆ†äº«å‡ºå»å–”ï¼\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: èŠ±è“®äººåŠ æ²¹ï¼ç½·å…æˆåŠŸé‚„æˆ‘æ´„ç€¾å®®ï¼\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: é‚„æœ‰å¿—å·¥åœ¨åœ“ç’°èˆ‰ç‰Œå®£å‚³ï¼Œè¾›è‹¦äº†!\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: äº†ä¸èµ·çš„èŠ±è“®ç½·åœ˜ï¼åŠ æ²¹ï¼\n",
      " -> é æ¸¬åˆ†é¡: æŠµåˆ¶èˆ‡è¡Œå‹•å‘¼ç±²\n",
      "\n",
      "æ–‡æœ¬: èŠ±è“®äººä¸€èµ·åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: èŠ±è“®äººåŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: èŠ±è“®äººåŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: èŠ±è“®äººåŠ æ²¹ï¼Œç«™å‡ºä¾†æ”¹è®ŠèŠ±è“®ï¼\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: èŠ±è“®åŠ æ²¹ è‡ºç£åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: å¹«æ¨\n",
      " -> é æ¸¬åˆ†é¡: åœ°æ–¹æˆ–æ”¿é»¨æ”¿æ²»æ‰¹è©•\n",
      "\n",
      "æ–‡æœ¬: åŠ æ²¹åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: ç™¼è¨€çš„å¤§å“¥å¤§å§Šå€‘ï¼Œå€‹å€‹äººé–“æ¸…é†’å•Š\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: \n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: åŠ æ²¹\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: åœ‹æ°‘é»¨æ„›å°ä¸æ„›ä¸­å…±çš„æ­£è—è»ï¼Œç«™å‡ºä¾†ç½·å…å‚…å´èæ•‘èŠ±è“®ã€‚\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "æ–‡æœ¬: è¾›è‹¦äº†\n",
      " -> é æ¸¬åˆ†é¡: å‰µæ„è¡¨é”èˆ‡è«·åˆº\n",
      "\n",
      "å·²å°‡çµæœå„²å­˜è‡³ï¼šC://Users//andyw//Desktop//æ¡‘é‘¼çš„åˆ†é¡//result1.xlsx\n"
     ]
    }
   ],
   "source": [
    "offset = 0\n",
    "for i in range(offset, 2):\n",
    "        \n",
    "    # 3. Tokenizer è™•ç†\n",
    "    # å‡è¨­ tokenizer å’Œ model å·²ç¶“åœ¨ä¸Šæ–¹è¼‰å…¥\n",
    "    # ä¾‹å¦‚ï¼š tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if len(all_list[i]) == 0:\n",
    "        continue\n",
    "    test_texts = pd.DataFrame(all_list[i])\n",
    "    test_texts['å…§å®¹'] = test_texts['å…§å®¹'].fillna(\"\").astype(str)\n",
    "    \n",
    "    input_sentences = test_texts['å…§å®¹'].tolist()  # è½‰ç‚º Python list\n",
    "\n",
    "    encoding = tokenizer(\n",
    "    input_sentences, \n",
    "    padding=\"longest\",\n",
    "    truncation=True,\n",
    "    max_length=512,        # common max length\n",
    "    return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # 4. æ¨è«– (Inference)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1).numpy()  # é€™è£¡æœƒæ˜¯æ•´æ•¸\n",
    "\n",
    "    # 5. å¯å°‡æ•´æ•¸é¡åˆ¥è½‰æˆã€Œæ–‡å­—æ¨™ç±¤ã€\n",
    "    predicted_labels = [id2label[p] for p in predictions]\n",
    "\n",
    "    # 6. å°‡é æ¸¬çµæœä¸€æ¬¡æ€§å¯«å› DataFrame\n",
    "    test_texts['åˆ†é¡'] = predicted_labels\n",
    "\n",
    "    # 7. (å¯é¸) åˆ—å°å‡ºçµæœæª¢æŸ¥\n",
    "    for text, label in zip(test_texts['å…§å®¹'], predicted_labels):\n",
    "        print(f\"æ–‡æœ¬: {text}\\n -> é æ¸¬åˆ†é¡: {label}\\n\")\n",
    "\n",
    "    # 8. å„²å­˜çµæœåˆ°æ–° Excel æª”\n",
    "    output_path = f\"C://Users//andyw//Desktop//æ¡‘é‘¼çš„åˆ†é¡//result{i}.xlsx\"\n",
    "    test_texts.to_excel(output_path, index=False)\n",
    "    print(f\"å·²å°‡çµæœå„²å­˜è‡³ï¼š{output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
