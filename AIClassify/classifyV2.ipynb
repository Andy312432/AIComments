{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# ç¢ºèªæ˜¯å¦æœ‰å¯ç”¨çš„GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®€å–CSVè³‡æ–™ï¼Œå‡è¨­åŒ…å« \"text\" èˆ‡ \"label\" æ¬„ä½\n",
    "df = pd.read_csv(\"referance_newCat.csv\")\n",
    "\n",
    "# å‡è¨­9å€‹åˆ†é¡æ¨™ç±¤æ˜¯ä»¥ä¸‹ï¼ˆè«‹æ ¹æ“šå¯¦éš›æƒ…æ³èª¿æ•´ï¼‰\n",
    "categories = [\n",
    "    \"äººèº«æ”»æ“Šèˆ‡ä¾®è¾±æ€§è¨€è«–\",\n",
    "    \"è³ªç–‘èº«ä»½èˆ‡èƒŒæ™¯\",\n",
    "    \"å­˜åœ¨æ„Ÿèˆ‡é—œæ³¨åº¦è©•è«–\",\n",
    "    \"æ”¿æ²»ç«‹å ´æ‰¹è©•\",\n",
    "    \"æ”¿æ²»ç«‹å ´è¡¨æ…‹\",\n",
    "    \"æ”¿æ²»é™°è¬€è«–èˆ‡æŒ‡æ§\",\n",
    "    \"æ”¿ç¸¾èˆ‡èƒ½åŠ›è³ªç–‘\",\n",
    "    \"é€ è¬ èˆ‡èª ä¿¡è³ªç–‘\",\n",
    "    \"åæŒ‡æ¨™èˆ‡è«·åˆºé æ¸¬\",\n",
    "    \"ç½·å…ç›¸é—œè©•è«–\",\n",
    "    \"é•å»ºè¾²èˆè­°é¡Œ\",\n",
    "    \"ç–«æƒ…èˆ‡ç–«è‹—ç›¸é—œè©•è«–\",\n",
    "    \"ç–‘ä¼¼æ©Ÿå™¨äººæˆ–è¤‡è£½è²¼ä¸Šç•™è¨€\",\n",
    "    \"å¹½é»˜èˆ‡å˜²è«·\",\n",
    "    \"ç°¡çŸ­æƒ…ç·’è¡¨é”\"\n",
    "]\n",
    "\n",
    "# å°‡æ–‡å­—æ¨™ç±¤è½‰æˆæ•¸å­—ID\n",
    "label2id = {cat: idx for idx, cat in enumerate(categories)}\n",
    "id2label = {idx: cat for cat, idx in label2id.items()}\n",
    "\n",
    "df['label_id'] = df['label'].apply(lambda x: label2id[x])  # å°‡æ–‡å­—æ¨™ç±¤è½‰æ•¸å­—\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨“ç·´ç­†æ•¸ï¼š 352\n",
      "æ¸¬è©¦ç­†æ•¸ï¼š 88\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label_id'], random_state=42)\n",
    "\n",
    "print(\"è¨“ç·´ç­†æ•¸ï¼š\", len(train_df))\n",
    "print(\"æ¸¬è©¦ç­†æ•¸ï¼š\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"hfl/chinese-roberta-wwm-ext-large\"  # ä½¿ç”¨æ›´å¤§çš„é è¨“ç·´æ¨¡å‹\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# å°‡æ–‡å­—è³‡æ–™è½‰æˆ BERT ç›¸å®¹çš„è¼¸å…¥æ ¼å¼\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"longest\",     # ä¹Ÿå¯ä»¥æ”¹æˆ longestï¼Œè‹¥æƒ³å‹•æ…‹paddingå¯ä½¿ç”¨ batched æ–¹å¼\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b147428c2abe4360b790222f1ef76a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/352 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c02da92450a4615af226fa0d604c898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/88 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'label_id']])\n",
    "test_dataset  = Dataset.from_pandas(test_df[['text', 'label_id']])\n",
    "\n",
    "# é€²è¡Œ Tokenize\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# ç§»é™¤å¤šé¤˜æ¬„ä½ï¼Œä¸¦æŒ‡å®šç‰¹å¾µåç¨±\n",
    "train_dataset = train_dataset.remove_columns([\"text\"])\n",
    "train_dataset = train_dataset.rename_column(\"label_id\", \"labels\")\n",
    "train_dataset.set_format(\"torch\")\n",
    "\n",
    "test_dataset = test_dataset.remove_columns([\"text\"])\n",
    "test_dataset = test_dataset.rename_column(\"label_id\", \"labels\")\n",
    "test_dataset.set_format(\"torch\")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = 15  # åˆ†é¡æ•¸é‡\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "# å°‡æ¨¡å‹ç§»å‹•åˆ°GPU\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andyw\\Desktop\\AIComments\\AIClassify\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",       # æ¯å€‹ epoch çµæŸå¾Œåšè©•ä¼°\n",
    "    save_strategy=\"epoch\",            # æ¯å€‹ epoch å„²å­˜ä¸€æ¬¡æ¨¡å‹ (å¯æ ¹æ“šéœ€æ±‚èª¿æ•´)\n",
    "    num_train_epochs=5,               # ä»¥è³‡æ–™é‡å°‘ç‚ºå‰æï¼Œå…ˆè©¦è©¦ 5 epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=3,    # è¦–ç¡¬é«”èª¿æ•´\n",
    "    per_device_eval_batch_size=3,\n",
    "    logging_dir=\"./logs\",             # è¨“ç·´éç¨‹ç´€éŒ„\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,       # åœ¨æœ€å¾Œè¼‰å…¥æœ€å¥½çš„æ¨¡å‹æ¬Šé‡\n",
    "    report_to=\"none\"                  # ç¦ç”¨å ±å‘Šåˆ°ä»»ä½•å¹³å°\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\"accuracy\": acc[\"accuracy\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_17760\\1215538910.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='473' max='590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [473/590 14:05 < 03:30, 0.56 it/s, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.621500</td>\n",
       "      <td>0.617520</td>\n",
       "      <td>0.795455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.305618</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.102700</td>\n",
       "      <td>0.200676</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.505900</td>\n",
       "      <td>0.239079</td>\n",
       "      <td>0.897727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 112, kind: StorageFull, message: \"ç£ç¢Ÿçš„ç©ºé–“ä¸è¶³ã€‚\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSafetensorError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m trainer = Trainer(\n\u001b[32m      2\u001b[39m     model=model,\n\u001b[32m      3\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# å–å¾—æœ€ä½³æ¨¡å‹å­˜æ”¾çš„ checkpoint è·¯å¾‘\u001b[39;00m\n\u001b[32m     13\u001b[39m best_checkpoint_dir = trainer.state.best_model_checkpoint\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\AIComments\\AIClassify\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\AIComments\\AIClassify\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2647\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2644\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2646\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2650\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2651\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\AIComments\\AIClassify\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3100\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[39m\n\u001b[32m   3097\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3099\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3100\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3101\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\AIComments\\AIClassify\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3197\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3195\u001b[39m run_dir = \u001b[38;5;28mself\u001b[39m._get_output_dir(trial=trial)\n\u001b[32m   3196\u001b[39m output_dir = os.path.join(run_dir, checkpoint_folder)\n\u001b[32m-> \u001b[39m\u001b[32m3197\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy \u001b[38;5;129;01min\u001b[39;00m [SaveStrategy.STEPS, SaveStrategy.EPOCH] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.best_global_step:\n\u001b[32m   3200\u001b[39m     best_checkpoint_folder = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPREFIX_CHECKPOINT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state.best_global_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\AIComments\\AIClassify\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3884\u001b[39m, in \u001b[36mTrainer.save_model\u001b[39m\u001b[34m(self, output_dir, _internal_call)\u001b[39m\n\u001b[32m   3881\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped.save_checkpoint(output_dir)\n\u001b[32m   3883\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3884\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3886\u001b[39m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[32m   3887\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.push_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\AIComments\\AIClassify\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3988\u001b[39m, in \u001b[36mTrainer._save\u001b[39m\u001b[34m(self, output_dir, state_dict)\u001b[39m\n\u001b[32m   3986\u001b[39m             torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n\u001b[32m   3987\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3988\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3989\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_safetensors\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3993\u001b[39m     \u001b[38;5;28mself\u001b[39m.processing_class.save_pretrained(output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\AIComments\\AIClassify\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3578\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   3573\u001b[39m     gc.collect()\n\u001b[32m   3575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[32m   3576\u001b[39m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[32m   3577\u001b[39m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3578\u001b[39m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3579\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3580\u001b[39m     save_function(shard, os.path.join(save_directory, shard_file))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\AIComments\\AIClassify\\.venv\\Lib\\site-packages\\safetensors\\torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mSafetensorError\u001b[39m: Error while serializing: IoError(Os { code: 112, kind: StorageFull, message: \"ç£ç¢Ÿçš„ç©ºé–“ä¸è¶³ã€‚\" })"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# å–å¾—æœ€ä½³æ¨¡å‹å­˜æ”¾çš„ checkpoint è·¯å¾‘\n",
    "best_checkpoint_dir = trainer.state.best_model_checkpoint\n",
    "print(\"Best model checkpoint folder:\", best_checkpoint_dir)\n",
    "\n",
    "# å¯ä»¥æŠŠæœ€ä½³æ¨¡å‹å¦å­˜åˆ°æŸå€‹è³‡æ–™å¤¾ï¼Œä¾‹å¦‚ \"./best_model\"\n",
    "best_model_dir = \"./best_model\"\n",
    "trainer.save_model(best_model_dir)\n",
    "print(f\"Best model is now saved to: {best_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_dir = \"./best_model\"\n",
    "model_name = \"hfl/chinese-roberta-wwm-ext-large\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(best_model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(best_model_dir)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_list = []\n",
    "\n",
    "year = '2022'\n",
    "#input_json_path = f\"C://Users//andyw//Desktop//æ¡‘é‘¼çš„åˆ†é¡//fb_comments//comments//result_{i}.json\"\n",
    "input_csv_path = f\"C://Users//andyw//Desktop//AIComments//AIClassify//comments_{year}.csv\"\n",
    "\n",
    "df = pd.read_csv(input_csv_path, header=None, names=[\"è²¼æ–‡æ™‚é–“\",\"å…§å®¹\", \"æ™‚é–“\"])\n",
    "\n",
    "#all_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4190/4190 [19:03<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡æœ¬: ä¸­å£¢äººç´ è³ªè¦å…ˆæ ½åŸ¹ï¼Œä¸ç„¶ç½·å…ç‹å¾Œï¼Œå†é¸å‡ºä¾†çš„é‚„æ˜¯ä¸€æ¨£\n",
      " -> é æ¸¬åˆ†é¡: åæŒ‡æ¨™èˆ‡è«·åˆºé æ¸¬\n",
      "\n",
      "æ–‡æœ¬: å…ˆè¿½ç©¶è±ªè¯é•å»ºå¤±è·å–®ä½ï¼Œç”­è®“ç‰ å¤ªå¥½éï¼\n",
      " -> é æ¸¬åˆ†é¡: ç½·å…ç›¸é—œè©•è«–\n",
      "\n",
      "æ–‡æœ¬: æˆ‘æ˜¯æ¡ƒåœ’çš„ï¼Œé›–æ²’æœ‰æŠ•ç¥¨æ¬Šï¼Œä½†æ˜¯ç²¾ç¥ä¸Šæ”¯æŒä¸­å£¢äººé †åˆ©ç½·å…æ‰æµªè²»ç´ç¨…äººä»˜è–ªæ°´çš„è­°å“¡\n",
      " -> é æ¸¬åˆ†é¡: åæŒ‡æ¨™èˆ‡è«·åˆºé æ¸¬\n",
      "\n",
      "æ–‡æœ¬: 0116ä¸­å£¢äººç«™å‡ºä¾†ï¼Œè«‹æŠ•ä¸‹è´Šæˆç¥¨ï¼Œçµ¦æµ©å®‡ä¸€å€‹é‡ç”Ÿçš„æ©Ÿæœƒï¼Œä¸­å£¢äººå¯«æ­·å²ï¼Œè«‹å„ä½å¹«å¿™æµ©å®‡æ‹‰ç¥¨ï¼Œéç‹ä¸æŠ•!!!\n",
      " -> é æ¸¬åˆ†é¡: ç–‘ä¼¼æ©Ÿå™¨äººæˆ–è¤‡è£½è²¼ä¸Šç•™è¨€\n",
      "\n",
      "æ–‡æœ¬: æœ‰è­°å“¡èº«åˆ†åœ¨ï¼Œæ¡ƒåœ’å¸‚åºœæ‹†é™¤å¤§éšŠä¸æ•¢æ‹†å•¦ï¼ç½·ç‹æ•‘è¾²åœ°å§ï¼\n",
      " -> é æ¸¬åˆ†é¡: ç½·å…ç›¸é—œè©•è«–\n",
      "\n",
      "å·²å°‡çµæœå„²å­˜è‡³ï¼šC://Users//andyw//Desktop//AIComments//AIClassify//resultComment//result2020.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 704/704 [03:12<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡æœ¬: åˆ°åº•ç½·å…é€£ç½²é–‹å§‹äº†æ²’ï¼Ÿé€™äººè¶Šçœ‹è¶Šè¨å­â‹¯â‹¯\n",
      " -> é æ¸¬åˆ†é¡: åæŒ‡æ¨™èˆ‡è«·åˆºé æ¸¬\n",
      "\n",
      "æ–‡æœ¬: å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ç¶ é»¨ç§˜æ›¸é•·å¼µç«¹èŠ©ï¼Œæµ©å®‡å·²ç¶“é€€é»¨ä¸€é™£å­äº†ï¼Œç¶ é»¨åŠªåŠ›åœ¨ç’°ä¿å’Œç¤¾æœƒæ­£ç¾©è­°é¡Œä¸Šè€•è€˜ï¼Œæ­¡è¿å¤§å®¶ä¾†é—œæ³¨ï¼#ç¶ é»¨ç§˜æ›¸é•·è·¯é\n",
      " -> é æ¸¬åˆ†é¡: ç–‘ä¼¼æ©Ÿå™¨äººæˆ–è¤‡è£½è²¼ä¸Šç•™è¨€\n",
      "\n",
      "æ–‡æœ¬: å°±æ˜¯é˜¿ï¼ è¶´è‘—çš‡å¤ªåçš„è…¿ ä¸æ˜¯å¤ªç›£å“ªæ˜¯ä»€éº¼ï¼Ÿï¼Ÿè‡ªå·±çš„é ­éŠœå°±å·²è­‰æ˜ä¸€åˆ‡****æ¡ƒåœ’å¸‚æ°‘é€²é»¨ç«¶é¸ç¸½éƒ¨å‰¯å¹¹äº‹*****\n",
      " -> é æ¸¬åˆ†é¡: é•å»ºè¾²èˆè­°é¡Œ\n",
      "\n",
      "æ–‡æœ¬: è¢«èªªå¤ªç›£ æ„Ÿè¦ºä»–å¥½åƒå¼•ä»¥ç‚ºæ¦®è€¶ï¼Ÿï¼Ÿ\n",
      " -> é æ¸¬åˆ†é¡: è³ªç–‘èº«ä»½èˆ‡èƒŒæ™¯\n",
      "\n",
      "æ–‡æœ¬: å…¶å¯¦æ˜¯å¥„äºº\n",
      " -> é æ¸¬åˆ†é¡: äººèº«æ”»æ“Šèˆ‡ä¾®è¾±æ€§è¨€è«–\n",
      "\n",
      "å·²å°‡çµæœå„²å­˜è‡³ï¼šC://Users//andyw//Desktop//AIComments//AIClassify//resultComment//result2019.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "all_list = []\n",
    "years = ['2020', '2019']\n",
    "\n",
    "for year in years:\n",
    "    #input_json_path = f\"C://Users//andyw//Desktop//æ¡‘é‘¼çš„åˆ†é¡//fb_comments//comments//result_{i}.json\"\n",
    "    input_csv_path = f\"C://Users//andyw//Desktop//AIComments//AIClassify//comments_{year}.csv\"\n",
    "\n",
    "    df = pd.read_csv(input_csv_path, header=None, names=[\"è²¼æ–‡æ™‚é–“\",\"å…§å®¹\", \"æ™‚é–“\"])\n",
    "\n",
    "    #all_list\n",
    "\n",
    "    \n",
    "\n",
    "    # (å‡è¨­ä½ å·²ç¶“å…ˆè¡Œè¼‰å…¥å®Œæ¨¡å‹å’Œ tokenizer)\n",
    "    # model_name = \"ä½ çš„æ¨¡å‹è·¯å¾‘æˆ–åç¨±\"\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    # å‡è¨­ test_texts æ˜¯ä¸€å€‹åŒ…å«ã€Œå…§å®¹ã€æ¬„ä½çš„ DataFrame\n",
    "    # e.g. test_texts = pd.read_excel(\"some_excel_file.xlsx\")\n",
    "    test_texts = df\n",
    "    test_texts['å…§å®¹'] = test_texts['å…§å®¹'].fillna(\"\").astype(str)\n",
    "    input_sentences = test_texts['å…§å®¹'].tolist()\n",
    "\n",
    "\n",
    "    # è¨­å®šæ‰¹æ¬¡å¤§å°\n",
    "    batch_size = 16\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    # åˆ†æ‰¹é€²è¡Œé æ¸¬ï¼Œä¸¦åœ¨è¿´åœˆå¤–åŒ…ä¸Š tqdm ä»¥é¡¯ç¤ºé€²åº¦\n",
    "    for i in tqdm(range(0, len(input_sentences), batch_size), desc=\"Predicting\"):\n",
    "        batch_texts = input_sentences[i : i + batch_size]\n",
    "\n",
    "        # Tokenizer\n",
    "        encoding = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "        batch_preds = torch.argmax(outputs.logits, dim=1).cpu().numpy().tolist()\n",
    "\n",
    "        # æ”¶é›†æ¯ä¸€æ‰¹çš„é æ¸¬çµæœ\n",
    "        all_predictions.extend(batch_preds)\n",
    "\n",
    "    # å°‡æ•¸å­—é æ¸¬çµæœè½‰æˆæ–‡å­—æ¨™ç±¤\n",
    "    predicted_labels = [id2label[p] for p in all_predictions]\n",
    "\n",
    "    # å°‡çµæœå¯«å›åŸæœ¬ DataFrame\n",
    "    test_texts['åˆ†é¡'] = predicted_labels\n",
    "\n",
    "    # (å¯é¸) æª¢æŸ¥é æ¸¬çµæœ\n",
    "    for text, label in zip(test_texts['å…§å®¹'][:5], test_texts['åˆ†é¡'][:5]):  # ç¯„ä¾‹åªå°å‰ 5 ç­†\n",
    "        print(f\"æ–‡æœ¬: {text}\\n -> é æ¸¬åˆ†é¡: {label}\\n\")\n",
    "\n",
    "    # å„²å­˜çµæœåˆ° Excel æª”\n",
    "    output_path = f\"C://Users//andyw//Desktop//AIComments//AIClassify//resultComment//result{year}.xlsx\"\n",
    "    test_texts.to_excel(output_path, index=False)\n",
    "    print(f\"å·²å°‡çµæœå„²å­˜è‡³ï¼š{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡æœ¬: ç†Ÿæ‚‰åˆåˆºè€³çš„è²éŸ³\n",
      " -> é æ¸¬åˆ†é¡: äººèº«æ”»æ“Šèˆ‡ä¾®è¾±æ€§è¨€è«–\n",
      "\n",
      "æ–‡æœ¬: å£äººä¸æœƒè®Šå¥½åªæœƒè®Šé†œ\n",
      " -> é æ¸¬åˆ†é¡: å¹½é»˜èˆ‡å˜²è«·\n",
      "\n",
      "æ–‡æœ¬: æœ‰å¥è©±èªªè¦çŸ¥é“ç‹—æ˜¯æ”¹ä¸äº†åƒæŸäº›é£Ÿç‰©çš„\n",
      " -> é æ¸¬åˆ†é¡: è³ªç–‘èº«ä»½èˆ‡èƒŒæ™¯\n",
      "\n",
      "æ–‡æœ¬: ä½ è¦ç¢ºå®šè€¶~XD\n",
      " -> é æ¸¬åˆ†é¡: ç°¡çŸ­æƒ…ç·’è¡¨é”\n",
      "\n",
      "æ–‡æœ¬: å†¤å®¶å®œè§£ä¸å®œçµ\n",
      " -> é æ¸¬åˆ†é¡: å¹½é»˜èˆ‡å˜²è«·\n",
      "\n",
      "å·²å°‡çµæœå„²å­˜è‡³ï¼šC://Users//andyw//Desktop//AIComments//AIClassify//resultComment//result2023.xlsx\n"
     ]
    }
   ],
   "source": [
    "#testtest\n",
    "\n",
    "\n",
    "# å°‡æ•¸å­—é æ¸¬çµæœè½‰æˆæ–‡å­—æ¨™ç±¤\n",
    "predicted_labels = [id2label[p] for p in all_predictions]\n",
    "\n",
    "# å°‡çµæœå¯«å›åŸæœ¬ DataFrame\n",
    "test_texts['åˆ†é¡'] = predicted_labels\n",
    "\n",
    "# (å¯é¸) æª¢æŸ¥é æ¸¬çµæœ\n",
    "for text, label in zip(test_texts['å…§å®¹'][:5], test_texts['åˆ†é¡'][:5]):  # ç¯„ä¾‹åªå°å‰ 5 ç­†\n",
    "    print(f\"æ–‡æœ¬: {text}\\n -> é æ¸¬åˆ†é¡: {label}\\n\")\n",
    "\n",
    "# å„²å­˜çµæœåˆ° Excel æª”\n",
    "output_path = \"C://Users//andyw//Desktop//AIComments//AIClassify//resultComment//result2023.xlsx\"\n",
    "test_texts.to_excel(output_path, index=False)\n",
    "print(f\"å·²å°‡çµæœå„²å­˜è‡³ï¼š{output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
