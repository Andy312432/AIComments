{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# 確認是否有可用的GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取CSV資料，假設包含 \"text\" 與 \"label\" 欄位\n",
    "df = pd.read_csv(\"referance.csv\")\n",
    "\n",
    "# 假設9個分類標籤是以下（請根據實際情況調整）\n",
    "categories = [\n",
    "    \"批評選民與動員\",\n",
    "    \"地方或政黨政治批評\",\n",
    "    \"人身攻擊言論\",\n",
    "    \"罷免立場表態\",\n",
    "    \"政策與程序討論\",\n",
    "    \"抵制與行動呼籲\",\n",
    "    \"創意表達與諷刺\",\n",
    "    \"國安及賣台指控\",\n",
    "    \"事件引用論述或加油的言論\",\n",
    "    \"其他\"\n",
    "]\n",
    "\n",
    "# 將文字標籤轉成數字ID\n",
    "label2id = {cat: idx for idx, cat in enumerate(categories)}\n",
    "id2label = {idx: cat for cat, idx in label2id.items()}\n",
    "\n",
    "df['label_id'] = df['label'].apply(lambda x: label2id[x])  # 將文字標籤轉數字\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練筆數： 283\n",
      "測試筆數： 71\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label_id'], random_state=42)\n",
    "\n",
    "print(\"訓練筆數：\", len(train_df))\n",
    "print(\"測試筆數：\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"hfl/chinese-macbert-large\"  # 使用更大的預訓練模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 將文字資料轉成 BERT 相容的輸入格式\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"longest\",     # 也可以改成 longest，若想動態padding可使用 batched 方式\n",
    "        truncation=True,\n",
    "        pad_to_max_length=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8b360580404ed8b8ba5068a1447179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2e453fb2d042168308304d8d4a25d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/71 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'label_id']])\n",
    "test_dataset  = Dataset.from_pandas(test_df[['text', 'label_id']])\n",
    "\n",
    "# 進行 Tokenize\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 移除多餘欄位，並指定特徵名稱\n",
    "train_dataset = train_dataset.remove_columns([\"text\"])\n",
    "train_dataset = train_dataset.rename_column(\"label_id\", \"labels\")\n",
    "train_dataset.set_format(\"torch\")\n",
    "\n",
    "test_dataset = test_dataset.remove_columns([\"text\"])\n",
    "test_dataset = test_dataset.rename_column(\"label_id\", \"labels\")\n",
    "test_dataset.set_format(\"torch\")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-macbert-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = 10  # 分類數量\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "# 將模型移動到GPU\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andyw\\Desktop\\桑鑼的分類\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",       # 每個 epoch 結束後做評估\n",
    "    save_strategy=\"epoch\",            # 每個 epoch 儲存一次模型 (可根據需求調整)\n",
    "    num_train_epochs=5,               # 以資料量少為前提，先試試 5 epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,    # 視硬體調整\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_dir=\"./logs\",             # 訓練過程紀錄\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,       # 在最後載入最好的模型權重\n",
    "    report_to=\"none\"                  # 禁用報告到任何平台\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\"accuracy\": acc[\"accuracy\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyw\\AppData\\Local\\Temp\\ipykernel_6340\\1215538910.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72' max='355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 72/355 09:21 < 37:50, 0.12 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.129800</td>\n",
       "      <td>1.867146</td>\n",
       "      <td>0.323944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m trainer = Trainer(\n\u001b[32m      2\u001b[39m     model=model,\n\u001b[32m      3\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 取得最佳模型存放的 checkpoint 路徑\u001b[39;00m\n\u001b[32m     13\u001b[39m best_checkpoint_dir = trainer.state.best_model_checkpoint\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\桑鑼的分類\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2241\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2239\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\桑鑼的分類\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2639\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2636\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2638\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2639\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2642\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2643\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\桑鑼的分類\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3092\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[39m\n\u001b[32m   3089\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3091\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3092\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3093\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\桑鑼的分類\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3194\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3190\u001b[39m \u001b[38;5;28mself\u001b[39m.save_model(output_dir, _internal_call=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_only_model:\n\u001b[32m   3193\u001b[39m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3194\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3195\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_scaler(output_dir)\n\u001b[32m   3196\u001b[39m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\桑鑼的分類\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3316\u001b[39m, in \u001b[36mTrainer._save_optimizer_and_scheduler\u001b[39m\u001b[34m(self, output_dir)\u001b[39m\n\u001b[32m   3311\u001b[39m     save_fsdp_optimizer(\n\u001b[32m   3312\u001b[39m         \u001b[38;5;28mself\u001b[39m.accelerator.state.fsdp_plugin, \u001b[38;5;28mself\u001b[39m.accelerator, \u001b[38;5;28mself\u001b[39m.optimizer, \u001b[38;5;28mself\u001b[39m.model, output_dir\n\u001b[32m   3313\u001b[39m     )\n\u001b[32m   3314\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m   3315\u001b[39m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3316\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3318\u001b[39m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[32m   3319\u001b[39m is_deepspeed_custom_scheduler = \u001b[38;5;28mself\u001b[39m.is_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m   3320\u001b[39m     \u001b[38;5;28mself\u001b[39m.lr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[32m   3321\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\桑鑼的分類\\.venv\\Lib\\site-packages\\torch\\serialization.py:944\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m    943\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m--> \u001b[39m\u001b[32m944\u001b[39m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    951\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andyw\\Desktop\\桑鑼的分類\\.venv\\Lib\\site-packages\\torch\\serialization.py:1216\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[39m\n\u001b[32m   1214\u001b[39m     storage = storage.cpu()\n\u001b[32m   1215\u001b[39m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1216\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# 取得最佳模型存放的 checkpoint 路徑\n",
    "best_checkpoint_dir = trainer.state.best_model_checkpoint\n",
    "print(\"Best model checkpoint folder:\", best_checkpoint_dir)\n",
    "\n",
    "# 可以把最佳模型另存到某個資料夾，例如 \"./best_model\"\n",
    "best_model_dir = \"./best_model\"\n",
    "trainer.save_model(best_model_dir)\n",
    "print(f\"Best model is now saved to: {best_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dir = \"./best_model_\"\n",
    "model_name = \"hfl/chinese-roberta-wwm-ext\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(best_model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(best_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "all_list = []\n",
    "#for i in range(48):\n",
    "for i in range(2):\n",
    "    #input_json_path = f\"C://Users//andyw//Desktop//桑鑼的分類//fb_comments//comments//result_{i}.json\"\n",
    "    input_json_path = f\"C://Users//andyw//Desktop//桑鑼的分類//fb_comments//result_video{i}.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            file_data = json.load(f) \n",
    "\n",
    "        row_list = []\n",
    "        for element in file_data:\n",
    "            row_list.append({\n",
    "                \"內容\": element[0],\n",
    "                \"時間\": element[1]\n",
    "            })\n",
    "        all_list.append(row_list)\n",
    "    except:#not found\n",
    "        all_list.append([])\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本: 連署書可以親送嗎？\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 高山風 您好，目前維持建議寄件到郵政信箱，但提醒可以使用掛號的方式寄出！相對保障！\n",
      " -> 預測分類: 事件引用論述或加油的言論\n",
      "\n",
      "文本: 罷完傅。去花蓮玩\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 黃國欽 歡迎唷\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 微光 shimmer.tw 加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 上街頭發連署書會不會更有效益？讓更多人知道更好取得？畢竟有年紀的人不一定會列印\n",
      " -> 預測分類: 國安及賣台指控\n",
      "\n",
      "文本: 洪靜蕙 會有的\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 洪靜蕙 規劃中\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 這邊有4份 很快寄出\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: Chen Hong Hong 謝謝\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: Chen Hong Hong \n",
      " -> 預測分類: 抵制與行動呼籲\n",
      "\n",
      "文本: 連署書怎麼領取???\n",
      " -> 預測分類: 事件引用論述或加油的言論\n",
      "\n",
      "文本: 趙玉芳 需要自行下載列印https://drive.google.com/....../1IevLDmwLJx....../view......\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 加油加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 花蓮加油！！！\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 每天來點布農語啊！mapasnava Bunun saikin 一起加油\n",
      " -> 預測分類: 事件引用論述或加油的言論\n",
      "\n",
      "文本: 花蓮人 加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: Happy Hsieh 花蓮人加油台灣加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 請問現在遷戶籍到花蓮來得及嗎？￼\n",
      " -> 預測分類: 事件引用論述或加油的言論\n",
      "\n",
      "文本: 加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 加油加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 江欣燕 加油加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 加油，已分享，我很前一批6月的不知道有沒有順利收到 XD\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: Hsiao Tung Chiu 一定是有的\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 請問早先八炯那次有連署了，還要再簽一次嗎?\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 花蓮人 加油!\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 用寄送的方式會不會因為某種原因沒送達啊??? 畢竟平信本來就沒保證一定送達\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 許少銓 不用擔心！真的擔心可以用「掛號」或「限掛」方式寄出！\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 我不是花蓮人，希望你們為了全民，大家看在眼裡，預祝成功。這個滯台的共產黨不除，台灣難平靜。自己做錯事被關，現在把仇恨附加在全民\n",
      " -> 預測分類: 國安及賣台指控\n",
      "\n",
      "文本: 60日是到什麼時候？過年拿回去給我爸媽簽\n",
      " -> 預測分類: 事件引用論述或加油的言論\n",
      "\n",
      "文本: 聽說第二階段很難\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 幫擴散到所有罷免團隊以下轉分享自「Kazuma Kawazoe 」脆貼文：剛剛做了 A3/A4 比例的告示牌如果有人來罷免活動騷擾，或許可派上用場/歡迎自由取用，我雲端資料夾設定為公開：https://drive.google.com/....../1JXwjMnp6mFsqnIBrT1stG....../我沒查到所有的公民罷免帳號，先 tag 各位～如果可以請幫我擴散，謝謝！@byebyecow777@shimmer.taiwan@songxin.recall2025@wanghongwei2025gg@daanreboothttps://www.threads.net/@youngandayu/post/DEbUR4Wzc9K......#越限縮人民罷免權代表藍白越害怕#各路罷免團隊一起加油#藍白紅都倒台灣才會好\n",
      " -> 預測分類: 事件引用論述或加油的言論\n",
      "\n",
      "文本: 大家一起努力加油！\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 花蓮人罷免衝起來，給下一代一個不被傅崑萁統治的機會\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 在那邊可以拿連署書\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 余文慶  可自行下載列印https://drive.google.com/....../1IevLDmwLJx....../view......\n",
      " -> 預測分類: 事件引用論述或加油的言論\n",
      "\n",
      "文本: 不能跟其他縣市一樣印好直接領取現在簽名就完成了，快速又方便\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: Hugo Chen 可自行下載列印會更快跟方便喔！至於大量印製某個程度上對沒有任何金援的團隊來說也會是一項負擔！…… 查看更多\n",
      " -> 預測分類: 國安及賣台指控\n",
      "\n",
      "已將結果儲存至：C://Users//andyw//Desktop//桑鑼的分類//result0.xlsx\n",
      "文本: 歡迎大家一起分享出去\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 大家一起分享出去喔！\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 花蓮人加油！罷免成功還我洄瀾宮！\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 還有志工在圓環舉牌宣傳，辛苦了!\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 了不起的花蓮罷團！加油！\n",
      " -> 預測分類: 抵制與行動呼籲\n",
      "\n",
      "文本: 花蓮人一起加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 花蓮人加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 花蓮人加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 花蓮人加油，站出來改變花蓮！\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 花蓮加油 臺灣加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 幫推\n",
      " -> 預測分類: 地方或政黨政治批評\n",
      "\n",
      "文本: 加油加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 發言的大哥大姊們，個個人間清醒啊\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: \n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 加油\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 國民黨愛台不愛中共的正藍軍，站出來罷免傅崐萁救花蓮。\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "文本: 辛苦了\n",
      " -> 預測分類: 創意表達與諷刺\n",
      "\n",
      "已將結果儲存至：C://Users//andyw//Desktop//桑鑼的分類//result1.xlsx\n"
     ]
    }
   ],
   "source": [
    "offset = 0\n",
    "for i in range(offset, 2):\n",
    "        \n",
    "    # 3. Tokenizer 處理\n",
    "    # 假設 tokenizer 和 model 已經在上方載入\n",
    "    # 例如： tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if len(all_list[i]) == 0:\n",
    "        continue\n",
    "    test_texts = pd.DataFrame(all_list[i])\n",
    "    test_texts['內容'] = test_texts['內容'].fillna(\"\").astype(str)\n",
    "    \n",
    "    input_sentences = test_texts['內容'].tolist()  # 轉為 Python list\n",
    "\n",
    "    encoding = tokenizer(\n",
    "    input_sentences, \n",
    "    padding=\"longest\",\n",
    "    truncation=True,\n",
    "    max_length=512,        # common max length\n",
    "    return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # 4. 推論 (Inference)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1).numpy()  # 這裡會是整數\n",
    "\n",
    "    # 5. 可將整數類別轉成「文字標籤」\n",
    "    predicted_labels = [id2label[p] for p in predictions]\n",
    "\n",
    "    # 6. 將預測結果一次性寫回 DataFrame\n",
    "    test_texts['分類'] = predicted_labels\n",
    "\n",
    "    # 7. (可選) 列印出結果檢查\n",
    "    for text, label in zip(test_texts['內容'], predicted_labels):\n",
    "        print(f\"文本: {text}\\n -> 預測分類: {label}\\n\")\n",
    "\n",
    "    # 8. 儲存結果到新 Excel 檔\n",
    "    output_path = f\"C://Users//andyw//Desktop//桑鑼的分類//result{i}.xlsx\"\n",
    "    test_texts.to_excel(output_path, index=False)\n",
    "    print(f\"已將結果儲存至：{output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
