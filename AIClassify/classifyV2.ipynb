{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# 確認是否有可用的GPU，盡可能用GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取CSV資料，包含 \"text\" 與 \"label\" 欄位\n",
    "df = pd.read_csv(\"referance_newCat.csv\")\n",
    "\n",
    "# 分類標籤\n",
    "categories = [\n",
    "    \"人身攻擊與侮辱性言論\",\n",
    "    \"質疑身份與背景\",\n",
    "    \"存在感與關注度評論\",\n",
    "    \"政治立場批評\",\n",
    "    \"政治立場表態\",\n",
    "    \"政治陰謀論與指控\",\n",
    "    \"政績與能力質疑\",\n",
    "    \"造謠與誠信質疑\",\n",
    "    \"反指標與諷刺預測\",\n",
    "    \"罷免相關評論\",\n",
    "    \"違建農舍議題\",\n",
    "    \"疫情與疫苗相關評論\",\n",
    "    \"疑似機器人或複製貼上留言\",\n",
    "    \"幽默與嘲諷\",\n",
    "    \"簡短情緒表達\"\n",
    "]\n",
    "\n",
    "# 將文字標籤轉成數字ID\n",
    "label2id = {cat: idx for idx, cat in enumerate(categories)}\n",
    "id2label = {idx: cat for cat, idx in label2id.items()}\n",
    "\n",
    "df['label_id'] = df['label'].apply(lambda x: label2id[x])  # 將文字標籤轉數字\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將資料集分為訓練集和測試集，並保持類別比例\n",
    "# 寫死random_state，讓每次分割都一樣\n",
    "# 可以確保每次訓練的資料集都是相同的，方便比較模型表現\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label_id'], random_state=42)\n",
    "\n",
    "print(\"訓練筆數：\", len(train_df))\n",
    "print(\"測試筆數：\", len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這邊其實測試時試了很多不同的模型，像是bert-base-chinese,hfl/chinese-roberta-wwm-ext, hfl/chinese-roberta-wwm-ext-large 等等\n",
    "# 但最後發現 hfl/chinese-roberta-wwm-ext-large 效果最好，雖然速度慢了一點，但準確率還是有提升\n",
    "model_name = \"hfl/chinese-roberta-wwm-ext-large\"  # 使用更大的預訓練模型，chatgpt推薦的，效果看起來也不差(比起原本的bert-base)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 將文字資料轉成 BERT 相容的輸入格式(以上的模型都是吃這個格式)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"longest\",     # 也可以改成 longest，若想動態padding可使用 batched 方式\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將資料轉成 Dataset 格式，這樣才能用 Trainer 進行訓練\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'label_id']])\n",
    "test_dataset  = Dataset.from_pandas(test_df[['text', 'label_id']])\n",
    "\n",
    "# 進行 Tokenize\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 移除多餘欄位，並指定特徵名稱\n",
    "train_dataset = train_dataset.remove_columns([\"text\"])\n",
    "train_dataset = train_dataset.rename_column(\"label_id\", \"labels\")\n",
    "train_dataset.set_format(\"torch\")\n",
    "\n",
    "test_dataset = test_dataset.remove_columns([\"text\"])\n",
    "test_dataset = test_dataset.rename_column(\"label_id\", \"labels\")\n",
    "test_dataset.set_format(\"torch\")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 15  # 分類數量\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "# 將模型移動到GPU\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定訓練參數，根據模型的Readme檔案，這邊的參數是參考他們的設定\n",
    "# 我的電腦batch size只能到3 (3050 laptop 30W)，可以根據自己的電腦調整\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",       # 每個 epoch 結束後做評估\n",
    "    save_strategy=\"epoch\",            # 每個 epoch 儲存一次模型 (可根據需求調整)\n",
    "    num_train_epochs=5,               # 以資料量少為前提，先試試 5 epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=3,    # 視硬體調整\n",
    "    per_device_eval_batch_size=3,\n",
    "    logging_dir=\"./logs\",             # 訓練過程紀錄\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,       # 在最後載入最好的模型權重\n",
    "    report_to=\"none\"                  \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#評估函數，評估每次epoch的準確率\n",
    "# 這邊使用了evaluate這個套件，這是huggingface官方的評估工具，可以算各種指標(by chatgpt)\n",
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\"accuracy\": acc[\"accuracy\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自動把短的句子(每筆測資經過tokenizer後，會變成一串串token)用 padding 補齊到一樣的長度，以便放入模型。但是前面tokenizer有改成另外一種形式動態padding，所以理論上不用這段的程式\n",
    "# 雖然是這樣說但是我這邊也沒有到很理解，該去細細研究一下NLP了(x\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將前面所有設定的參數塞進去Trainer裡面\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# 取得最佳模型存放的 checkpoint 路徑\n",
    "best_checkpoint_dir = trainer.state.best_model_checkpoint\n",
    "print(\"Best model checkpoint folder:\", best_checkpoint_dir)\n",
    "\n",
    "# 把最佳模型另存到某個資料夾，例如 \"./best_model\"\n",
    "best_model_dir = \"./best_model\"\n",
    "trainer.save_model(best_model_dir)\n",
    "print(f\"Best model is now saved to: {best_model_dir}\")\n",
    "\n",
    "#到此預訓練結束，接下來是使用模型的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#預訓練完成後，之後想要使用模型的話，只要載入模型就可以了\n",
    "#理論上若不需要重新訓練的話，可以直接從這邊開始執行(當然前面要import)\n",
    "\n",
    "best_model_dir = \"./best_model\"\n",
    "model_name = \"hfl/chinese-roberta-wwm-ext-large\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(best_model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(best_model_dir)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取測試資料，格式對應之前爬蟲出來的資料\n",
    "import pandas as pd\n",
    "\n",
    "all_list = []\n",
    "\n",
    "year = '2022'\n",
    "#input_json_path = f\"C://Users//andyw//Desktop//桑鑼的分類//fb_comments//comments//result_{i}.json\"\n",
    "input_csv_path = f\"C://Users//andyw//Desktop//AIComments//AIClassify//comments_{year}.csv\"\n",
    "\n",
    "df = pd.read_csv(input_csv_path, header=None, names=[\"貼文時間\",\"內容\", \"時間\"])\n",
    "\n",
    "#all_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "all_list = []\n",
    "years = ['2020', '2019']\n",
    "\n",
    "for year in years:\n",
    "    #input_json_path = f\"C://Users//andyw//Desktop//桑鑼的分類//fb_comments//comments//result_{i}.json\"\n",
    "    input_csv_path = f\"C://Users//andyw//Desktop//AIComments//AIClassify//comments_{year}.csv\"\n",
    "\n",
    "    df = pd.read_csv(input_csv_path, header=None, names=[\"貼文時間\",\"內容\", \"時間\"])\n",
    "\n",
    "\n",
    "    # (假設你已經先行載入完模型和 tokenizer)\n",
    "    # model_name = \"你的模型路徑或名稱\"\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    # 假設 test_texts 是一個包含「內容」欄位的 DataFrame\n",
    "    # e.g. test_texts = pd.read_excel(\"some_excel_file.xlsx\")\n",
    "    #將內容欄位的空值填上空字串，並轉成字串格式(確保資料一致)\n",
    "    test_texts = df\n",
    "    test_texts['內容'] = test_texts['內容'].fillna(\"\").astype(str)\n",
    "    input_sentences = test_texts['內容'].tolist()\n",
    "\n",
    "\n",
    "    # 設定批次大小\n",
    "    batch_size = 16\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    # 分批進行預測，並在迴圈外包上 tqdm 以顯示進度(by chatgpt)\n",
    "    for i in tqdm(range(0, len(input_sentences), batch_size), desc=\"Predicting\"):\n",
    "        batch_texts = input_sentences[i : i + batch_size]\n",
    "\n",
    "        # Tokenizer\n",
    "        encoding = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "        batch_preds = torch.argmax(outputs.logits, dim=1).cpu().numpy().tolist()\n",
    "\n",
    "        # 收集每一批的預測結果\n",
    "        all_predictions.extend(batch_preds)\n",
    "\n",
    "    # 將數字預測結果轉成文字標籤\n",
    "    predicted_labels = [id2label[p] for p in all_predictions]\n",
    "\n",
    "    # 將結果寫回原本 DataFrame\n",
    "    test_texts['分類'] = predicted_labels\n",
    "\n",
    "    # (可選) 檢查預測結果\n",
    "    for text, label in zip(test_texts['內容'][:5], test_texts['分類'][:5]):  # 範例只印前 5 筆\n",
    "        print(f\"文本: {text}\\n -> 預測分類: {label}\\n\")\n",
    "\n",
    "    # 儲存結果到 Excel 檔\n",
    "    output_path = f\"C://Users//andyw//Desktop//AIComments//AIClassify//resultComment//result{year}.xlsx\"\n",
    "    test_texts.to_excel(output_path, index=False)\n",
    "    print(f\"已將結果儲存至：{output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
